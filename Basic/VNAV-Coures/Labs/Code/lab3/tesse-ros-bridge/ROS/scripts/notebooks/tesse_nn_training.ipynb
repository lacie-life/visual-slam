{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESSE NN Training Script\n",
    "\n",
    "Use this to collect many rgb and segmented images within the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesse.env import Env\n",
    "from tesse.msgs import *\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tf.transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get spawn points from json file\n",
    "\n",
    "The spawn-point file is located in `tesse_human_0x/tesse_human_0x_DATA/StreamingAssets/tesse_multiscene_[scene].points`\n",
    "\n",
    "Replace `x` with the version; as of June 30 2020 the latest version is 09. Replace `[scene]` with the environment. For the office scene, this is \"office2\".\n",
    "\n",
    "Second cell will tell you how many spawn points are available for the agent and each class of human. You may spawn more humans that there are spawn points; in this case humans will spawn on top of each other and disperse once commanded to move. Command humans to move by clicking on the simulator screen.\n",
    "\n",
    "Note that you can capture additional spawn points for the agent by using spawn-capture-mode. More details [here](https://github.mit.edu/TESS/tesse-core#agent-spawn-points). Spawn yaw-angle is randomized so even with limited spawn points you can have novel images just by having different orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spawn_point_file = \"\"\n",
    "spawn_points_df = pd.read_json(spawn_point_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_points = []\n",
    "female_points = []\n",
    "male_points = []\n",
    "\n",
    "for i in range(len(spawn_points_df)):\n",
    "    name = spawn_points_df.iloc[i]['spawnPoints']['name']\n",
    "    \n",
    "    for point in spawn_points_df.iloc[i]['spawnPoints']['points']:\n",
    "        if name == \"tesse_multiscene_agent\":\n",
    "            agent_points.append(np.array([point[j] for j in ['x','y','z']]))\n",
    "        elif name == \"SMPL_Female_Autonomous\":\n",
    "            female_points.append(np.array([point[j] for j in ['x','y','z']]))\n",
    "        elif name == \"SMPL_Male_Autonomous\":\n",
    "            male_points.append(np.array([point[j] for j in ['x','y','z']]))\n",
    "\n",
    "print(\"NOTE: \" + str(len(agent_points)) + \" points are available for image capture.\")\n",
    "print(\"NOTE: \" + str(len(female_points)) + \" points are available for female spawning.\")\n",
    "print(\"NOTE: \" + str(len(male_points)) + \" points are available for male spawning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup simulator\n",
    "\n",
    "Note: make sure simulator executable is running now. Set `scene_id` to 1 for the office scene.\n",
    "\n",
    "Check to make sure the cameras are set up properly, and choose how many humans to spawn into the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = 1\n",
    "\n",
    "env = Env()\n",
    "_ = env.request(SceneRequest(scene_id))\n",
    "\n",
    "cameras=[\n",
    "    (Camera.RGB_LEFT, Compression.OFF, Channels.THREE),\n",
    "    (Camera.SEGMENTATION, Compression.OFF, Channels.THREE),\n",
    "]\n",
    "\n",
    "# These are the same settings used for uHumans2!\n",
    "fov = 60\n",
    "width = 720\n",
    "height = 480\n",
    "near_draw_dist = -0.05\n",
    "far_draw_dist = 50\n",
    "pos = [-0.05, 0, 0]\n",
    "quat = [0, 0, 0, 1]\n",
    "\n",
    "for camera in cameras:\n",
    "    _ = env.request(SetCameraParametersRequest(camera[0], height, width, fov, near_draw_dist, far_draw_dist))\n",
    "\n",
    "    response = env.request(CameraInformationRequest(camera[0]))\n",
    "    if response is not None:\n",
    "        print(response.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_females = 10\n",
    "num_males = num_females\n",
    "\n",
    "for i in range(num_females):\n",
    "    _ = env.request(SpawnObjectRequest(0, ObjectSpawnMethod.RANDOM))\n",
    "\n",
    "for i in range(num_males):\n",
    "    _ = env.request(SpawnObjectRequest(1, ObjectSpawnMethod.RANDOM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "`save_img` and `show_img` are self-explanatory. Note that if `show_img` is enabled, you will have to click through images to advance to the next capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = 10\n",
    "save_img = True\n",
    "show_img = False\n",
    "rgb_img_output_dir = \"\"\n",
    "seg_img_output_dir = \"\"\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    point = agent_points[np.random.randint(0, len(agent_points))]\n",
    "    yaw = np.random.random() * 2.0 * np.pi\n",
    "    quat = tf.transformations.quaternion_from_euler(0., yaw, 0.)\n",
    "    \n",
    "    env.send(Reposition(point[0], point[1], point[2], *quat))\n",
    "    \n",
    "    response = env.request(DataRequest(metadata=False, cameras=cameras))\n",
    "    if response:\n",
    "        # incoming encoding is bgr!\n",
    "        img_rgb = cv2.cvtColor(response.images[0], cv2.COLOR_BGR2RGB)\n",
    "        img_seg = cv2.cvtColor(response.images[1], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if save_img:\n",
    "            filename_rgb = os.path.join(rgb_img_output_dir, \"img_rgb_\" + str(i) + \".jpg\")\n",
    "            filename_seg = os.path.join(seg_img_output_dir, \"img_seg_\" + str(i) + \".jpg\")\n",
    "            cv2.imwrite(filename_rgb, img_rgb)\n",
    "            cv2.imwrite(filename_seg, img_seg)\n",
    "\n",
    "        if show_img:\n",
    "            for img in [img_rgb, img_seg]:\n",
    "                cv2.imshow('img', img)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
